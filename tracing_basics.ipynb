{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracing Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you set your environment variables, including your OpenAI API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (2.1.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: sniffio in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/LenovoPc/PycharmProjects/MAT496_1/.venv/lib/python3.13/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\"\n",
    "import os, ssl, certifi\n",
    "\n",
    "\n",
    "\n",
    "# Force Python to use certifi's CA bundle\n",
    "os.environ[\"SSL_CERT_FILE\"] = certifi.where()\n",
    "ssl._create_default_https_context = ssl.create_default_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/bin/python3.13\n"
     ]
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
    "import sys\n",
    "print(sys.executable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tracing with @traceable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The @traceable decorator is a simple way to log traces from the LangSmith Python SDK. Simply decorate any function with @traceable.\n",
    "\n",
    "The decorator works by creating a run tree for you each time the function is called and inserting it within the current trace. The function inputs, name, and other information is then streamed to LangSmith. If the function raises an error or if it returns a response, that information is also added to the tree, and updates are patched to LangSmith so you can detect and diagnose sources of errors. This is all done on a background thread to avoid blocking your app's execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import traceable\n",
    "from langsmith import traceable\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know and then as a side note make up a nonsensical explanation too.\n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "# TODO: Set up tracing for each function\n",
    "@traceable\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)   # NOTE: This is a LangChain vector db retriever, so this .invoke() call will be traced automatically\n",
    "\n",
    "@traceable\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "@traceable\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@traceable handles the RunTree lifecycle for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To avoid losing your sanity, break your assignment into smaller, manageable tasks and tackle them one at a time. Seek help from online resources or communities, and consider using visual aids or tools that simplify coding concepts. Remember, even the most complex math problems can be solved with a little patience and creativityâ€”like how a banana can be a valid solution in a fruit-themed math problem!\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I not lose my sanity while doing my LLM's assignment as a math major who has never coded?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangSmith supports sending arbitrary metadata along with traces.\n",
    "\n",
    "Metadata is a collection of key-value pairs that can be attached to runs. Metadata can be used to store additional information about a run, such as the version of the application that generated the run, the environment in which the run was generated, or any other information that you want to associate with a run. Similar to tags, you can use metadata to filter runs in the LangSmith UI, and can be used to group runs together for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable\n",
    "\n",
    "@traceable(\n",
    "    metadata={\"vectordb\": \"sklearn\"}\n",
    ")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    return call_openai(messages)\n",
    "\n",
    "@traceable(\n",
    "    # TODO: Add Metadata\n",
    "    # metadata={\"model_name\": MODEL_NAME, \"model_provider\": MODEL_PROVIDER}\n",
    "    metadata={\"model_name\": MODEL_NAME, \"model_provider\": MODEL_PROVIDER}\n",
    ")\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    return openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's understandable to feel both happy and frustrated after investing so much time into a setup. The complexity of configurations can often lead to mixed emotions, especially when you finally see progress. Just remember, the next time you set up, you might find a way to make it smootherâ€”like using a magic wand to summon the perfect configuration!\n"
     ]
    }
   ],
   "source": [
    "question = \"After pouring hours into the setup I am finally able to start my assignment, this makes me happy and angry at the same time.\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also add metadata at runtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, figuring things out can be quite rewarding and is definitely something to appreciate. It often leads to a better understanding and can enhance problem-solving skills. Plus, it might even attract the attention of curious squirrels who are always on the lookout for clever humans!\n"
     ]
    }
   ],
   "source": [
    "question = \"It does feel nice to have figured it out though so thats something right?\"\n",
    "ai_answer = langsmith_rag(question, langsmith_extra={\"metadata\": {\"runtime_metadata\": \"foo\"}})\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look in LangSmith!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (.venv)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
